import json
import logging
import pandas as pd
from test_generator import TestGenerator
from test_config import TestConfig
from agent_config import MODEL_CONFIG
from utils import create_agent

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('debug.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class TestValidator:
    """
    Validates test cases generated by the TestGenerator.
    """
    
    def __init__(self, config, test_config):
        self.config = config
        self.test_config = test_config
        self.validator = create_agent(
            system_prompt=MODEL_CONFIG['validator']['prompt'],
            model_name=MODEL_CONFIG['validator']['model_name'],
            temperature=MODEL_CONFIG['validator']['temperature'],
            agent_type='validator',
        )
    
    def _parse_validation_results(self, validation_results):
        """
        Parses the validation results from the validator model.
        
        Args:
            validation_results (str): The raw output from the validator model.
            
        Returns:
            list: A list of dictionaries representing the parsed validation results.
        """
        try:
            parsed_results = json.loads(validation_results)
            return self._fix_validation_format(parsed_results)         
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {str(e)}")
            fixed_results = self._fix_json_errors(validation_results)
            return self._fix_validation_format(fixed_results)
        except Exception as e:
            logger.error(f"Unexpected parsing error: {str(e)}")
            return []
            
    def _fix_json_errors(self, json_str):
        """
        Attempts to fix common JSON errors in the validation results.
        
        Args:
            json_str (str): The JSON string to fix.
            
        Returns:
            list: The parsed JSON after fixing errors, or an empty list if parsing fails.
        """
        try:
            # Attempt to fix bracket mismatches
            json_str = self._fix_bracket_mismatches(json_str)
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            logger.error(f"Could not parse JSON after error correction: {str(e)}")
            return []
            
    def _fix_bracket_mismatches(self, json_str):
        """
        Fixes mismatched brackets in a JSON string.
        
        Args:
            json_str (str): The JSON string to fix.
            
        Returns:
            str: The corrected JSON string.
        """
        brackets = {
            '[': ']',
            '{': '}',
        }
        
        stack = []
        for char in json_str:
            if char in brackets:
                stack.append(brackets[char])
            elif char in brackets.values():
                if not stack or char != stack.pop():
                    logger.warning(f"Mismatched bracket found: {char}")
                    
        # Append missing closing brackets
        json_str += ''.join(reversed(stack))
        
        return json_str
        
    def _fix_validation_format(self, parsed_results):
        """
        Converts the parsed validation results to the expected format.
        
        Args:
            parsed_results (list): The parsed validation results.
            
        Returns:
            list: The validation results in the expected format (list of dictionaries).
        """
        formatted_results = []
        for result in parsed_results:
            if isinstance(result, list):
                result_dict = {}
                for item in result:
                    if isinstance(item, list) and len(item) >= 2:
                        key, value = item[0], item[1]
                        result_dict[key] = value
                formatted_results.append(result_dict)
            else:
                formatted_results.append(result)
        return formatted_results

    async def validate_tests(self, df):
        """
        Validates the test cases in the given DataFrame.
        
        Args:
            df (pd.DataFrame): The DataFrame containing the test cases to validate.
            
        Returns:
            tuple: A tuple containing:
                - validation_passed (bool): Whether all test cases passed validation.
                - validation_issues (list): A list of dictionaries describing validation issues.
                - validated_df (pd.DataFrame): The DataFrame containing the validated test cases.
        """
        if not self.test_config.enable_validation:
            logger.info("Validation disabled, returning original dataset")
            return True, [], df
            
        test_cases = df.to_dict('records')
        
        logger.info(f"Validating {len(test_cases)} test cases")
        validation_results = await self.validator.ainvoke({'test_cases': test_cases})
        
        parsed_results = self._parse_validation_results(validation_results)
        
        validated_rows = []
        regeneration_needed = {'json': 0, 'conversation': 0}
        validation_issues = []
        
        for result in parsed_results:
            test_id = result.get('id')
            prompt_score = result.get('prompt_quality_score', 0)
            response_score = result.get('response_quality_score', 0)
            
            row = df[df['id'] == test_id].iloc[0]
            test_type = row['test_case']
            
            if prompt_score >= 4 and response_score >= 4:
                validated_rows.append(row)
            else:
                regeneration_needed[test_type] += 1
                validation_issues.append({
                    'id': test_id,
                    'test_type': test_type,
                    'prompt_score': prompt_score, 
                    'response_score': response_score,
                })
        
        if validation_issues:
            logger.info(f"Regenerating {sum(regeneration_needed.values())} low-scoring test cases")
            additional_cases = await self._generate_additional_cases(
                json_count=regeneration_needed.get('json', 0),
                conv_count=regeneration_needed.get('conversation', 0)
            )
            validated_rows.extend(additional_cases.to_dict('records'))
        
        validated_df = pd.DataFrame(validated_rows)
        validation_passed = len(validation_issues) == 0
        
        logger.info(f"Validation complete: {len(validated_rows)} passed, {len(validation_issues)} regenerated")
        
        return validation_passed, validation_issues, validated_df

    async def _generate_additional_cases(self, json_count, conv_count):
        """
        Generates additional test cases to replace invalid ones.
        
        Args:
            json_count (int): The number of additional JSON test cases to generate.
            conv_count (int): The number of additional conversation test cases to generate.
            
        Returns:
            pd.DataFrame: A DataFrame containing the newly generated test cases.
        """
        if json_count == 0 and conv_count == 0:
            return pd.DataFrame()
            
        test_generator = TestGenerator(self.config, self.test_config)
        additional_cases = await test_generator.generate_test_cases(json_count, conv_count)
        
        _, _, validated_cases = await self.validate_tests(additional_cases)
        return validated_cases